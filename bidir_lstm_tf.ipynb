{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB = 20000\n",
    "SEQ_LEN = 100\n",
    "TRAIN_DIR = './aclImdb/train'\n",
    "TEST_DIR = './aclImdb/test'\n",
    "VOCAB_DIR = './aclImdb/imdb.vocab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab():\n",
    "    word_index = {r'\\unknow':0}\n",
    "    with tf.gfile.GFile(VOCAB_DIR) as f:        \n",
    "        for i in range(1, MAX_VOCAB):\n",
    "            word = f.readline().strip()\n",
    "            word_index[word] = i\n",
    "    return word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_texts(directory, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'):\n",
    "    \n",
    "    x = []\n",
    "    trans_table = str.maketrans(dict.fromkeys(filters))\n",
    "\n",
    "    for fname in os.listdir(directory):\n",
    "        with tf.gfile.GFile(os.path.join(directory, fname)) as f:\n",
    "            text = f.read()\n",
    "            text = re.sub(r'<.{,6}?/>', ' ', text).strip().lower()            \n",
    "            text = text.translate(trans_table)\n",
    "            x.append(text)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory):\n",
    "    \n",
    "    x_pos = load_texts(os.path.join(directory, 'pos'))\n",
    "    x_neg = load_texts(os.path.join(directory, 'neg'))\n",
    "    \n",
    "    y_pos = np.array([1]*len(x_pos), dtype=np.bool)\n",
    "    y_neg = np.array([0]*len(x_neg), dtype=np.bool)\n",
    "    \n",
    "    x = np.concatenate((x_pos, x_neg))\n",
    "    y = np.concatenate((y_pos, y_neg))\n",
    "    \n",
    "    return x, y        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_sequences(texts, word_index):\n",
    "    \n",
    "    x = []\n",
    "    \n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        x.append([word_index.get(word) if word in word_index else 0 for word in words])\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, maxlen, value=0):\n",
    "    \n",
    "    x = np.empty((len(sequences), maxlen), dtype=np.uint16)\n",
    "    x.fill(value)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        if len(seq) >= maxlen:\n",
    "            x[i] = seq[:maxlen]\n",
    "        else:            \n",
    "            x[i,:len(seq)] = seq\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(x, y, batch_size=32, shuffle=True):\n",
    "    n_samples = x.shape[0]\n",
    "    if shuffle:\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        x = x[indices]\n",
    "        y = y[indices]\n",
    "    \n",
    "    x_batched = []\n",
    "    y_batched = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < n_samples - batch_size:\n",
    "        x_batched.append(x[i:i+batch_size])\n",
    "        y_batched.append(y[i:i+batch_size])\n",
    "        i += batch_size\n",
    "    \n",
    "    x_batched.append(x[i:])\n",
    "    y_batched.append(y[i:])\n",
    "    \n",
    "    return x_batched, y_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(inputs):\n",
    "    \n",
    "    embedding_matrix = tf.get_variable('embedding_matrix', [20000, 128],\\\n",
    "                                       initializer=tf.truncated_normal_initializer(stddev=0.05))\n",
    "    \n",
    "    word_embeddings = tf.nn.embedding_lookup(embedding_matrix, inputs)\n",
    "    \n",
    "    cell_fw = tf.nn.rnn_cell.LSTMCell(128, initializer=tf.truncated_normal_initializer(stddev=0.05))\n",
    "    cell_bw = tf.nn.rnn_cell.LSTMCell(128, initializer=tf.truncated_normal_initializer(stddev=0.05))\n",
    "    \n",
    "    x, state = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, word_embeddings, dtype=tf.float32)\n",
    "    \n",
    "    x = tf.concat(x, -1)\n",
    "    x = tf.reshape(x, [-1, x.shape[1]*x.shape[2]])\n",
    "    x = tf.layers.dense(x, 1024, activation=tf.nn.relu,\\\n",
    "                        kernel_initializer=tf.truncated_normal_initializer(stddev=0.05),\\\n",
    "                        kernel_regularizer=tf.contrib.layers.l2_regularizer)\n",
    "    \n",
    "    logits = tf.layers.dense(x, 2, \n",
    "                             kernel_initializer=tf.truncated_normal_initializer(stddev=0.05),\\\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = load_vocab()\n",
    "x_train, y_train = load_data(TRAIN_DIR)\n",
    "x_train = texts_to_sequences(x_train, word_index)\n",
    "x_train = pad_sequences(x_train, maxlen=SEQ_LEN)\n",
    "x_train, y_train = batch(x_train, y_train, 128)\n",
    "\n",
    "x_test, y_test = load_data(TEST_DIR)\n",
    "x_test = texts_to_sequences(x_test, word_index)\n",
    "x_test = pad_sequences(x_test, maxlen=SEQ_LEN)\n",
    "x_test, y_test = batch(x_test, y_test, 128, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.int32, shape=[None, SEQ_LEN], name='x')\n",
    "y = tf.placeholder(tf.int32, shape=[None,], name='y')\n",
    "\n",
    "logits = my_model(x)\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "correct_pred = tf.equal(tf.argmax(logits, axis=1, output_type=tf.int32), y)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss_train: 0.6872 , acc_train: 0.5530 , loss_val: 0.6800 , acc_val: 0.5923\n",
      "epoch 2: loss_train: 0.6629 , acc_train: 0.6337 , loss_val: 0.6515 , acc_val: 0.6364\n",
      "epoch 3: loss_train: 0.6144 , acc_train: 0.6788 , loss_val: 0.5992 , acc_val: 0.6796\n",
      "epoch 4: loss_train: 0.5510 , acc_train: 0.7204 , loss_val: 0.5519 , acc_val: 0.7156\n",
      "epoch 5: loss_train: 0.4956 , acc_train: 0.7602 , loss_val: 0.5179 , acc_val: 0.7389\n",
      "epoch 6: loss_train: 0.4502 , acc_train: 0.7902 , loss_val: 0.4942 , acc_val: 0.7586\n",
      "epoch 7: loss_train: 0.4139 , acc_train: 0.8116 , loss_val: 0.4786 , acc_val: 0.7696\n",
      "epoch 8: loss_train: 0.3845 , acc_train: 0.8272 , loss_val: 0.4683 , acc_val: 0.7772\n",
      "epoch 9: loss_train: 0.3598 , acc_train: 0.8435 , loss_val: 0.4613 , acc_val: 0.7824\n",
      "epoch 10: loss_train: 0.3381 , acc_train: 0.8544 , loss_val: 0.4567 , acc_val: 0.7872\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        \n",
    "        loss_train = []\n",
    "        corr_pred_train = []\n",
    "        \n",
    "        loss_val = []\n",
    "        corr_pred_val = []\n",
    "        \n",
    "        for batch_x, batch_y in zip(x_train, y_train):\n",
    "            op, cos, corr = sess.run((optimizer, cost, correct_pred), feed_dict={x:batch_x, y:batch_y})\n",
    "            loss_train.append(cos)\n",
    "            corr_pred_train.append(corr)\n",
    "        \n",
    "        for batch_x, batch_y in zip(x_test, y_test):\n",
    "            cos, corr = sess.run((cost, correct_pred), feed_dict={x:batch_x, y:batch_y})\n",
    "            loss_val.append(cos)\n",
    "            corr_pred_val.append(corr)\n",
    "            \n",
    "        acc_train = np.concatenate(tuple(corr_pred_train), axis=0).mean()\n",
    "        acc_val = np.concatenate(tuple(corr_pred_val), axis=0).mean()\n",
    "        print('epoch {}: loss_train: {:.4f} , acc_train: {:.4f}'\\\n",
    "              .format(epoch, np.mean(loss_train), acc_train), end=' , ')\n",
    "        print('loss_val: {:.4f} , acc_val: {:.4f}'.format(np.mean(loss_val), acc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
